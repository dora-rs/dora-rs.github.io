"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[4416],{25665:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"nodes_operators/yolop_op","title":"yolop operator","description":"yolop recognizes lanes, and drivable area from a specific images.","source":"@site/docs/nodes_operators/yolop_op.md","sourceDirName":"nodes_operators","slug":"/nodes_operators/yolop_op","permalink":"/docs/nodes_operators/yolop_op","draft":false,"unlisted":false,"editUrl":"https://github.com/dora-rs/dora-rs.github.io/edit/main/docs/nodes_operators/yolop_op.md","tags":[],"version":"current","frontMatter":{}}');var r=o(74848),a=o(28453);const s={},i="yolop operator",l={},d=[{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Example plot ( lanes in red, drivable area in green)",id:"example-plot--lanes-in-red-drivable-area-in-green",level:2},{value:"Graph Description",id:"graph-description",level:2},{value:"Methods",id:"methods",level:2},{value:"<code>__init__()</code>",id:"__init__",level:3},{value:"<code>.on_event(...)</code>",id:"on_event",level:3},{value:"<code>.on_input(...)</code>",id:"on_input",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"yolop-operator",children:"yolop operator"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"yolop"})," recognizes lanes, and drivable area from a specific images."]}),"\n",(0,r.jsxs)(n.p,{children:["More info here: ",(0,r.jsx)(n.a,{href:"https://github.com/hustvl/YOLOP",children:"https://github.com/hustvl/YOLOP"})]}),"\n",(0,r.jsx)(n.p,{children:"You can also choose to allocate the model in GPU using the environment variable:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"PYTORCH_DEVICE: cuda # or cpu"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"inputs",children:"Inputs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"image: HEIGHT x WIDTH x BGR array."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"outputs",children:"Outputs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"drivable_area: drivable area as contour points"}),"\n",(0,r.jsx)(n.li,{children:"lanes: lanes as 60 points representing the lanes"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"example-plot--lanes-in-red-drivable-area-in-green",children:"Example plot ( lanes in red, drivable area in green)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://i.imgur.com/I531NIT.gif",alt:"Imgur"})}),"\n",(0,r.jsx)(n.h2,{id:"graph-description",children:"Graph Description"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"  - id: yolop\n    operator: \n      outputs:\n        - lanes\n        - drivable_area\n      inputs:\n        image: webcam/image\n      python: ../../operators/yolop_op.py\n"})}),"\n",(0,r.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,r.jsx)(n.h3,{id:"__init__",children:(0,r.jsx)(n.code,{children:"__init__()"})}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Source Code"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'    def __init__(self):\n        self.model = torch.hub.load("hustvl/yolop", "yolop", pretrained=True)\n        self.model.to(torch.device(DEVICE))\n        self.model.eval()\n\n\n'})})]}),"\n",(0,r.jsx)(n.h3,{id:"on_event",children:(0,r.jsx)(n.code,{children:".on_event(...)"})}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Source Code"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'\n    def on_event(\n        self,\n        dora_event: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        if dora_event["type"] == "INPUT":\n            return self.on_input(dora_event, send_output)\n        return DoraStatus.CONTINUE\n\n\n'})})]}),"\n",(0,r.jsx)(n.h3,{id:"on_input",children:(0,r.jsx)(n.code,{children:".on_input(...)"})}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Source Code"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'\n    def on_input(\n        self,\n        dora_input: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        # inference\n        frame = cv2.imdecode(\n            np.frombuffer(\n                dora_input["data"],\n                np.uint8,\n            ),\n            -1,\n        )\n\n        frame = frame[:, :, :3]\n        h0, w0, _ = frame.shape\n        h, w = (640, 640)\n        frame, _, (pad_w, pad_h) = letterbox_for_img(frame)\n        ratio = w / w0\n        pad_h, pad_w = (int(pad_h), int(pad_w))\n\n        img = torch.unsqueeze(transform(frame), dim=0)\n        half = False  # half precision only supported on CUDA\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img = img.to(torch.device(DEVICE))\n        det_out, da_seg_out, ll_seg_out = self.model(img)\n\n        # det_out = [pred.reshape((1, -1, 6)) for pred in det_out]\n        # inf_out = torch.cat(det_out, dim=1)\n\n        # det_pred = non_max_suppression(\n        # inf_out,\n        # )\n        # det = det_pred[0]\n\n        da_predict = da_seg_out[:, :, pad_h : (h0 - pad_h), pad_w : (w0 - pad_w)]\n        da_seg_mask = torch.nn.functional.interpolate(\n            da_predict, scale_factor=1 / ratio, mode="bilinear"\n        )\n        _, da_seg_mask = torch.max(da_seg_mask, 1)\n        da_seg_mask = da_seg_mask.int().squeeze().cpu().numpy()\n        da_seg_mask = morphological_process(da_seg_mask, kernel_size=7)\n\n        contours, _ = cv2.findContours(\n            da_seg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n        if len(contours) != 0:\n            contour = max(contours, key=cv2.contourArea)\n            contour = contour.astype(np.int32)\n            send_output("drivable_area", contour.tobytes(), dora_input["metadata"])\n        else:\n            send_output("drivable_area", np.array([]).tobytes(), dora_input["metadata"])\n\n        ll_predict = ll_seg_out[:, :, pad_h : (h0 - pad_h), pad_w : (w0 - pad_w)]\n\n        ll_seg_mask = torch.nn.functional.interpolate(\n            ll_predict, scale_factor=1 / ratio, mode="bilinear"\n        )\n\n        _, ll_seg_mask = torch.max(ll_seg_mask, 1)\n        ll_seg_mask = ll_seg_mask.int().squeeze().cpu().numpy()\n        # Lane line post-processing\n        ll_seg_mask = morphological_process(\n            ll_seg_mask, kernel_size=7, func_type=cv2.MORPH_OPEN\n        )\n        ll_seg_points = np.array(connect_lane(ll_seg_mask), np.int32)\n        send_output("lanes", ll_seg_points.tobytes(), dora_input["metadata"])\n        return DoraStatus.CONTINUE\n\n\n'})})]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>i});var t=o(96540);const r={},a=t.createContext(r);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);