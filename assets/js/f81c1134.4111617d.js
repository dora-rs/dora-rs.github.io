"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[8130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/reachy-pick-place","metadata":{"permalink":"/blog/reachy-pick-place","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/reachy-pick-place.md","source":"@site/blog/reachy-pick-place.md","title":"Reachy2 Pick and Place","description":"Using reachy2 with QwenVL2.5 to pick object and put it in a brown bag.","date":"2025-07-30T10:59:38.000Z","tags":[],"readingTime":0.9,"hasTruncateMarker":false,"authors":[{"name":"Haixuan Xavier Tao","title":"Maintainer of dora-rs","url":"https://github.com/haixuantao","imageURL":"https://github.com/haixuantao.png","key":"haixuan","page":null}],"frontMatter":{"authors":"haixuan","title":"Reachy2 Pick and Place","description":"Using reachy2 with QwenVL2.5 to pick object and put it in a brown bag."},"unlisted":false,"nextItem":{"title":"Reachy2 Speech-to-Grasp","permalink":"/blog/reachy-qwenvl-sam2"}},"content":"##### Using qwenVL 2.5 multi bounding box capabilities to pick and place mulitple item with very low latency.\\n\\n## Rerun\\n\\n<iframe src=\\"https://app.rerun.io/version/0.21.0/index.html?url=https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/final_chocolate_in_hand.rrd\\" width=\\"100%\\" height=\\"700px\\"></iframe>\\n\\n> In case Rerun does not work on your phone. You\'ll find the video below:\\n> <video controls src=\\"https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/2025-02-25%2020-19-46.mp4#t=10\\" width=\\"100%\\"  ></video>\\n> In the above iframe, the important information are:\\n\\n- `/text_whisper`: correspond to **whisper** audio transcription.\\n- `/text_response`: correspond to the bounding box given as plain text from **QwenVL 2.5**\\n- `camera_torso`: correspond to **Orbecc Gemini 336 Depth Camera** rgb image.\\n- `camera_torso` bounding box: correspond to the QwenVL bounding box projected on the image that is going to be used to grasp object. The prediction is done at regular interval and does not disappear. Sorry if it can be a bit confusing.\\n\\n## Code\\n\\nThe branch: https://github.com/dora-rs/dora/pull/793\\n\\n## Demo\\n\\n<video controls src=\\"https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/temp_video_1740509513726.mp4\\" width=\\"100%\\"  ></video>"},{"id":"/reachy-qwenvl-sam2","metadata":{"permalink":"/blog/reachy-qwenvl-sam2","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/reachy-qwenvl-sam2.md","source":"@site/blog/reachy-qwenvl-sam2.md","title":"Reachy2 Speech-to-Grasp","description":"Using reachy2 with QwenVL2 and SAM2 in order to create a robust universal* grasper. *We currently does not optimise for grasp pose, but it is going to come","date":"2025-07-30T10:59:38.000Z","tags":[],"readingTime":4.31,"hasTruncateMarker":false,"authors":[{"name":"Haixuan Xavier Tao","title":"Maintainer of dora-rs","url":"https://github.com/haixuantao","imageURL":"https://github.com/haixuantao.png","key":"haixuan","page":null}],"frontMatter":{"authors":"haixuan","title":"Reachy2 Speech-to-Grasp","description":"Using reachy2 with QwenVL2 and SAM2 in order to create a robust universal* grasper. *We currently does not optimise for grasp pose, but it is going to come"},"unlisted":false,"prevItem":{"title":"Reachy2 Pick and Place","permalink":"/blog/reachy-pick-place"},"nextItem":{"title":"Rust-Python FFI","permalink":"/blog/rust-python"}},"content":"Reachy grasping demo showcase how by combining multiple AI models together we can create a robot able to grasp object on a table from our speech autonomously.\\n\\nThis is performed with 100% open source code.\\n\\nThis approach also has the advantage of not be dependent on any environemnt finetuning meaning it can work pretty much anywhere with any robot out of the box.\\n\\n:::warning\\n\\n## Grasping limitation\\n\\n- Grasping is limited to small concave object that fits in reachy\'s claws.\\n- Grasping always has a fixed rotation angle pose.\\n- Current trajectory are predetermined.\\n\\nBy universal grasping, we want to focus on the fact that the object can be any object as long as we can define it using a generalistic prompt as opposed to previous approach dependant on predefined label.\\n\\n:::\\n\\n## Main ideas\\n\\n- Convert audio to a sequence using **Silero VAD**\\n- Convert the sequence to text using **OpenAI Whisper**\\n- Convert the user\'s text and the rgb image from **Orbecc Gemini 336 Camera (camera torso)** into a bounding box using **QwenVL 2.5**\\n- Convert the bounding box into a masks using **Meta SAM2**.\\n- Convert the masks and the depth image of **Orbecc Gemini 336 Camera** into a position using efficient **rust** code powered by **dora-rs.**\\n- Go to the position using inverse kinematics provided by **Pollen Robotics**.\\n- Go to predetermined position and come back in a scripted way. (In the future we plan to automate this. )\\n\\n---\\n\\n## Demo features that are technically not necessary\\n\\n- We have added head movement by searching for people when idle.\\n- We have made the head look at the predicted bounding box when the object is identified.\\n- We have made the robot turn left/right and releasing object into the box in a predetermined way in order to make people imagine follow up possibility.\\n- We have used both arms of the robot also using only one would also work, although, the grasping area given a fixed mobile base would be greatly reduced.\\n\\n## Source Code\\n\\nAll the source code and instructions are contained within dora-rs repository: https://github.com/dora-rs/dora/pull/784\\n\\nThe code is 100% open source and is aimed at being 100% reusable on other hardware using dora-rs by just replacing: `reachy-left-arm, reachy-right-arm, reachy-camera and reachy-head`. Albeit, porting this code might not be 100% easy as of now, and further work need to be done.\\n\\n## Computer requirements\\n\\nThis run on both:\\n\\n- MacOS with ~ 20G of RAM but without SAM2 as SAM2 is only available on nvidia for now.\\n- Linux with ~ 10G of Nvidia VRAM and ~16G of Nvidia VRAM using SAM2.\\n- I have not tried Windows but should run as well.\\n\\n## Annex: Rerun\\n\\n<iframe src=\\"https://app.rerun.io/version/0.21.0/index.html?url=https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/final_data.rrd\\" width=\\"100%\\" height=\\"700px\\"></iframe>\\n\\nIn the above iframe, the important information are:\\n\\n- `/text_whisper`: correspond to **whisper** audio transcription.\\n- `/text_response`: correspond to the bounding box given as plain text from **QwenVL 2.5**\\n- `camera_torso`: correspond to **Orbecc Gemini 336 Depth Camera** rgb image.\\n- `camera_torso` bounding box: correspond to the QwenVL bounding box projected on the image that is going to be used to grasp object. The prediction is done at regular interval and does not disappear. Sorry if it can be a bit confusing.\\n- `camera_left` bounding box: correspond to the QwenVL bounding box for detecting humans for head movement and is strictly a gimmick feature.\\n\\n## Annex: Graph of nodes running in parallel\\n\\n```mermaid\\n    flowchart TB\\n    dora-microphone[\\"**dora-microphone**\\"]\\n    sam2[\\"**sam2**\\"]\\n    dora-vad[\\"**dora-vad**\\"]\\n    dora-distil-whisper[\\"**dora-distil-whisper**\\"]\\n    reachy-mobile-base[\\"**reachy-mobile-base**\\"]\\n    reachy-left-arm[\\"**reachy-left-arm**\\"]\\n    reachy-right-arm[\\"**reachy-right-arm**\\"]\\n    reachy-camera[\\"**reachy-camera**\\"]\\n    reachy-head[/\\"**reachy-head**\\"\\\\]\\n    plot[/\\"**plot**\\"\\\\]\\n    dora-qwenvl[\\"**dora-qwenvl**\\"]\\n    parse_bbox[\\"**parse_bbox**\\"]\\n    box_coordinates[\\"**box_coordinates**\\"]\\n\\n    state_machine[\\"**state_machine**\\"]\\n    subgraph ___dora___ [dora]\\n    subgraph ___timer_timer___ [timer]\\n    dora/timer/millis/50[\\\\millis/50/]\\n    dora/timer/millis/500[\\\\millis/500/]\\n    dora/timer/secs/2[\\\\secs/2/]\\n    end\\n    end\\n    dora/timer/secs/2 -- tick --\x3e dora-microphone\\n    parse_bbox -- bbox as boxes2d --\x3e sam2\\n    reachy-camera -- image_depth --\x3e sam2\\n    dora-microphone -- audio --\x3e dora-vad\\n    dora-vad -- audio as input --\x3e dora-distil-whisper\\n    state_machine -- action_base --\x3e reachy-mobile-base\\n    state_machine -- action_l_arm as pose --\x3e reachy-left-arm\\n    state_machine -- action_r_arm as pose --\x3e reachy-right-arm\\n    dora/timer/millis/50 -- tick --\x3e reachy-camera\\n    parse_bbox -- bbox_face as boxes2d --\x3e reachy-head\\n    state_machine -- look --\x3e reachy-head\\n    parse_bbox -- bbox_face as camera_left/boxes2d_face --\x3e plot\\n    reachy-camera -- image_right as camera_left/image_right --\x3e plot\\n    parse_bbox -- bbox as camera_torso/boxes2d --\x3e plot\\n    reachy-camera -- image_depth as camera_torso/image --\x3e plot\\n    dora-qwenvl -- text as text_response --\x3e plot\\n    dora-distil-whisper -- text as text_whisper --\x3e plot\\n    reachy-camera -- image_depth --\x3e dora-qwenvl\\n    reachy-camera -- image_left --\x3e dora-qwenvl\\n    dora/timer/millis/500 -- text_1 --\x3e dora-qwenvl\\n    state_machine -- text_vlm as text_2 --\x3e dora-qwenvl\\n    dora-qwenvl -- text --\x3e parse_bbox\\n    reachy-camera -- depth --\x3e box_coordinates\\n    sam2 -- masks --\x3e box_coordinates\\n    box_coordinates -- pose --\x3e state_machine\\n    reachy-mobile-base -- response_base --\x3e state_machine\\n    reachy-left-arm -- response_l_arm --\x3e state_machine\\n    reachy-right-arm -- response_r_arm --\x3e state_machine\\n    dora-distil-whisper -- text --\x3e state_machine\\n```\\n\\nThe above graph correspond to the exact communication channels between the nodes that are running concurrently.\\n\\n## Annex: Video\\n\\n<video controls src=\\"https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/761752120155.mp4\\" width=\\"100%\\"  ></video>\\n\\nIn case you want to hire or buy reachy, you can send him an email at reachy@1ms.ai"},{"id":"/rust-python","metadata":{"permalink":"/blog/rust-python","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/rust-python.md","source":"@site/blog/rust-python.md","title":"Rust-Python FFI","description":"Rust-Python FFI.","date":"2025-07-30T10:59:38.000Z","tags":[],"readingTime":11.59,"hasTruncateMarker":false,"authors":[{"name":"Haixuan Xavier Tao","title":"Maintainer of dora-rs","url":"https://github.com/haixuantao","imageURL":"https://github.com/haixuantao.png","key":"haixuan","page":null}],"frontMatter":{"authors":"haixuan","title":"Rust-Python FFI","description":"Rust-Python FFI."},"unlisted":false,"prevItem":{"title":"Reachy2 Speech-to-Grasp","permalink":"/blog/reachy-qwenvl-sam2"}},"content":"Writing a rust library that is usable in multiple languages is not easy...\\n\\nThis blogpost recollects things I have encountered while building [wonnx](https://github.com/webonnx/wonnx) and [dora-rs](https://github.com/dora-rs/dora). I am going to use Rust-Python FFI through `pyo3` as an example. You can then extrapolate those issues to other languages FFI.\\n\\n## Foreign Function Interface\\n\\nA foreign function interface (FFI) is an interface used to share data from different languages. \\n\\nBy default, python might not know what a Rust `u16` is, so an interface is needed to make the two languages communicate.\\n\\n![](https://hackmd.io/_uploads/S1qiK8hRh.png)\\n> Image from [WebAssembly Interface Types: Interoperate with All the Things!](https://hacks.mozilla.org/2019/08/webassembly-interface-types/)\\n\\nBuilding interfaces is not easy. Most of the time, we have to use the C-ABI to build our FFI as it is the common denominator between languages.\\n\\nThankfully, there are FFI libraries that create interfaces for us and we can just focus on the important stuff such as the logic, algorithm, and so on.\\n\\nHowever, those FFI libraries might have limitations. This is what we\'re going to discuss.\\n\\nOne example of such FFI library is [`pyo3`](https://github.com/PyO3/pyo3). [`pyo3`](https://github.com/PyO3/pyo3) is one of the most used Rust-Python binding and creates FFIs for you. All we have to do is wrap our function with a `#[pyfunction]` and that will make it usable in Python.\\n\\n## Interfacing Arrays\\n\\nIn this blog post, I\'m going to build a toy Rust-Python project with `pyo3` to illustrate the issues I have faced.\\n\\nYou can try this blogpost at home by forking the [blogpost repository](https://github.com/haixuanTao/blogpost_ffi).\\n\\nIf you want to start from scratch, you can create a new project with:\\n\\n```bash\\nmkdir blogpost_ffi\\nmaturin init # pyo3\\n```\\n\\nThe default project will looks like this:\\n\\n```rust\\nuse pyo3::prelude::*;\\n\\n/// Formats the sum of two numbers as string.\\n#[pyfunction]\\nfn sum_as_string(a: usize, b: usize) -> PyResult<String> {\\n    Ok((a + b).to_string())\\n}\\n\\n/// A Python module implemented in Rust. The name of this function must match\\n/// the `lib.name` setting in the `Cargo.toml`, else Python will not be able to\\n/// import the module.\\n#[pymodule]\\nfn string_sum(_py: Python<\'_>, m: &PyModule) -> PyResult<()> {\\n    m.add_function(wrap_pyfunction!(sum_as_string, m)?)?;\\n    Ok(())\\n}\\n```\\nWe can call the function as follows:\\n```bash\\nmaturin develop\\npython -c \\"import blogpost_ffi; print(blogpost_ffi.sum_as_string(1,1))\\"\\n# Return: \\"2\\" \\n```\\n\\nIn the above example, `pyo3` is going to create FFIs to make Python integer interpretable as a Rust `usize` without additional work. \\n\\nHowever, automatically interpreted types might not be the most optimized implementation.\\n\\n\\n### Implementation 1: Default\\n\\nLet\'s imagine that, we want to play with arrays, we want to receive an array input and return an array output between Rust and Python.\\nA default inplementation, would look like this:\\n\\n\\n```rust    \\n#[pyfunction]\\nfn create_list(a: Vec<&PyAny>) -> PyResult<Vec<&PyAny>> {\\n    Ok(a)\\n}\\n\\n#[pymodule]\\nfn blogpost_ffi(_py: Python, m: &PyModule) -> PyResult<()> {\\n    m.add_function(wrap_pyfunction!(sum_as_string, m)?)?;\\n    m.add_function(wrap_pyfunction!(create_list, m)?)?;\\n    Ok(())\\n}\\n\\n```\\n\\n#### > Calling `create_list` for a very large list like: `value = [1] * 100_000_000`  is going to return in **2.27s** :tractor: \\n\\nThat\'s quite slow... The reason being is that this list is going to be interpret one element at a time in a loop. We can do better by trying to use all elements at the same time.\\n\\n> Check [test_script.py](https://github.com/haixuanTao/blogpost_ffi/blob/main/test_script.py) for details on how the function is called.\\n\\n### Implementation 2: PyBytes\\n\\nLet\'s imagine that our array is a C-contiguous array that can be represented as a [`PyBytes`](https://docs.python.org/3/library/stdtypes.html?highlight=bytes#bytes). The code can be optimized by casting the inputs and output as a `PyBytes`:\\n```rust\\n#[pyfunction]\\nfn create_list_bytes<\'a>(py: Python<\'a>, a: &\'a PyBytes) -> PyResult<&\'a PyBytes> {\\n    let s = a.as_bytes();\\n\\n    let output = PyBytes::new_with(py, s.len(), |bytes| {\\n        bytes.copy_from_slice(s);\\n        Ok(())\\n    })?;\\n    Ok(output)\\n}\\n```\\n\\n#### > For the same list input, `create_list_bytes` returns in **78 milliseconds**. That\'s **30x** better :racehorse: \\n\\n\\nThe speedup comes from the possibility to copy the memory range instead of iterating each element and to read without copying.\\n\\nNow the issue is that:\\n- `PyBytes` is only available in Python meaning that if we plan to have other languages, we will have to replicate this for each language.\\n- `PyBytes` might also probably need to be reconverted into other useful types.\\n- `PyBytes` needs a copy to be created.\\n\\nWe can try to solve this with [Apache Arrow](https://arrow.apache.org/).\\n\\n### Implementation 3: [Apache Arrow](https://arrow.apache.org/)\\n\\nApache Arrow is a universal memory format available in many languages. \\n\\nThe same function in `arrow` would look like this:\\n```rust \\n#[pyfunction]\\nfn create_list_arrow(py: Python, a: &PyAny) -> PyResult<Py<PyAny>> {\\n    let arraydata = arrow::array::ArrayData::from_pyarrow(a).unwrap();\\n\\n    let buffer = arraydata.buffers()[0].as_slice();\\n    let len = buffer.len();\\n\\n    // Zero Copy Buffer reference counted\\n    let arc_s = Arc::new(buffer.to_vec());\\n    let ptr = NonNull::new(arc_s.as_ptr() as *mut _).unwrap();\\n    let raw_buffer = unsafe { arrow::buffer::Buffer::from_custom_allocation(ptr, len, arc_s) };\\n    let output = arrow::array::ArrayData::try_new(\\n        arrow::datatypes::DataType::UInt8,\\n        len,\\n        None,\\n        0,\\n        vec![raw_buffer],\\n        vec![],\\n    )\\n    .unwrap();\\n\\n    output.to_pyarrow(py)\\n}\\n\\n```\\n\\n\\n#### > Same list returns in **33 milliseconds** . That\'s **2x** better than `PyBytes` :racehorse::racehorse: \\n\\n\\nThis is due to having zero copy when sending back the result. The zero-copying is safe because we are reference-counting the array. The array will be deallocating once all reference has been removed. \\n\\nThe benefits of `arrow` is:\\n- to make zero-copy achievable, scaling better with bigger data.\\n- being reusable in other languages. We only have to replace the last line of the function with the export to the other languages. \\n- having many types description including `List`,`Mapping` and `Struct`.\\n- being directly usable in `numpy`, `pandas`, and `pytorch` with zero-copy transmutation.\\n\\n## Debugging\\n\\nDealing with efficient Interface is not the only challenge of bridging multiple languages. We also have to deal with cross-language debugging. \\n\\n### `.unwrap()`\\n\\nOur current implementation uses `.unwrap()`. However, this will panic the whole Python process if there is an error. \\n\\n#### > Example error:\\n```bash\\nthread \'<unnamed>\' panicked at \'called `Result::unwrap()` on an `Err` value: PyErr { type: <class \'TypeError\'>, value: TypeError(\'Expected instance of pyarrow.lib.Array, got builtins.int\'), traceback: None }\', src/lib.rs:45:62\\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\\nTraceback (most recent call last):\\n  File \\"/home/peter/Documents/work/blogpost_ffi/test_script.py\\", line 79, in <module>\\n    array = blogpost_ffi.create_list_arrow(1)\\npyo3_runtime.PanicException: called `Result::unwrap()` on an `Err` value: PyErr { type: <class \'TypeError\'>, value: TypeError(\'Expected instance of pyarrow.lib.Array, got builtins.int\'), traceback: None }\\n```\\n\\n### [eyre](https://github.com/eyre-rs/eyre)\\n\\nEyre is an easy idiomatic error handling library for Rust applications. We can use eyre by wrapping our `pyo3` project with the `pyo3/eyre` feature flag, to replace all our `.unwrap()` with a `.context(\\"our context\\")?`. This will transform unrecoverable errors into recoverable Python errors while giving details about our errors.\\n\\n#### > Same error as above but with `eyre` which gives a better looking error message:\\n```bash\\nCould not convert arrow data\\n\\nCaused by:\\n    TypeError: Expected instance of pyarrow.lib.Array, got builtins.int\\n\\nLocation:\\n    src/lib.rs:75:50\\n```\\n\\nImplementation details:\\n```rust\\n#[pyfunction]\\nfn create_list_arrow_eyre(py: Python, a: &PyAny) -> Result<Py<PyAny>> {\\n    let arraydata =\\n        arrow::array::ArrayData::from_pyarrow(a).context(\\"Could not convert arrow data\\")?;\\n\\n    let buffer = arraydata.buffers()[0].as_slice();\\n    let len = buffer.len();\\n\\n    // Zero Copy Buffer reference counted\\n    let arc_s = Arc::new(buffer.to_vec());\\n    let ptr = NonNull::new(arc_s.as_ptr() as *mut _).context(\\"Could not create pointer\\")?;\\n    let raw_buffer = unsafe { arrow::buffer::Buffer::from_custom_allocation(ptr, len, arc_s) };\\n    let output = arrow::array::ArrayData::try_new(\\n        arrow::datatypes::DataType::UInt8,\\n        len,\\n        None,\\n        0,\\n        vec![raw_buffer],\\n        vec![],\\n    )\\n    .context(\\"could not create arrow arraydata\\")?;\\n\\n    output\\n        .to_pyarrow(py)\\n        .context(\\"Could not convert to pyarrow\\")\\n}\\n\\n```\\n\\n### Python traceback with `eyre`\\n\\nI will mention that you might lose the Python traceback error when calling Python code from a Rust code.\\n\\nI recommend using the following custom traceback method to have a descriptive error:\\n```rust\\n#[pyfunction]\\nfn call_func_eyre(py: Python, func: Py<PyAny>) -> Result<()> {    \\n    let _call_python = func.call0(py).context(\\"function called failed\\")?;\\n    Ok(())\\n}\\n\\nfn traceback(err: pyo3::PyErr) -> eyre::Report {\\n    let traceback = Python::with_gil(|py| err.traceback(py).and_then(|t| t.format().ok()));\\n    if let Some(traceback) = traceback {\\n        eyre::eyre!(\\"{traceback}\\\\n{err}\\")\\n    } else {\\n        eyre::eyre!(\\"{err}\\")\\n    }\\n}\\n\\n#[pyfunction]\\nfn call_func_eyre_traceback(py: Python, func: Py<PyAny>) -> Result<()> {\\n    let _call_python = func\\n        .call0(py)\\n        .map_err(traceback) // this will gives python traceback.\\n        .context(\\"function called failed\\")?;\\n    Ok(())\\n}\\n```\\n\\n#### > Example error with no custom traceback:\\n\\n```\\n---Eyre no traceback---\\neyre no traceback says: function called failed\\n\\nCaused by:\\n    AssertionError: I have no idea what is wrong\\n\\nLocation:\\n    src/lib.rs:89:39\\n------\\n```\\n\\n#### > Better errors with custom traceback:\\n\\n```\\n---Eyre traceback---\\neyre traceback says: function called failed\\n\\nCaused by:\\n    Traceback (most recent call last):\\n      File \\"/home/peter/Documents/work/blogpost_ffi/test_script.py\\", line 96, in abc\\n        assert False, \\"I have no idea what is wrong\\"\\n\\n    AssertionError: I have no idea what is wrong\\n\\nLocation:\\n    src/lib.rs:96:9\\n------\\n```\\n\\nWith the traceback, we can quickly identify the root error. \\n\\n## Memory management\\n\\nLet\'s take another example, and imagine that we need to create arrays within a loop:\\n\\n```rust\\n/// Unbounded memory growth\\n#[pyfunction]\\nfn unbounded_memory_growth(py: Python) -> Result<()> {\\n    for _ in 0..10 {\\n        let a: Vec<u8> = vec![0; 40_000_000];\\n        let _ = PyBytes::new(py, &a);`\\n        \\n        std::thread::sleep(Duration::from_secs(1));\\n    }\\n\\n    Ok(())\\n```\\n\\n#### > Calling this function will consume 440MB of memory. :-1: \\n\\nWhat happened is that `pyo3` memory model keeps all Python variables in memory until the GIL is released.\\n\\nTherefore, if we create variables in a `pyfunction` loop, all temporary variables are going to be kept until the GIL is released. \\n\\nThis is due to `pyfunction` locking the GIL by default.\\n\\nBy understanding the GIL-based memory model, we can use a scoped GIL to have the expected behaviour:\\n\\n```rust\\n#[pyfunction]\\nfn bounded_memory_growth(py: Python) -> Result<()> {\\n    py.allow_threads(|| {\\n        for _ in 0..10 {\\n            Python::with_gil(|py| {\\n                let a: Vec<u8> = vec![0; 40_000_000];\\n                let _bytes = PyBytes::new(py, &a);\\n            \\n                std::thread::sleep(Duration::from_secs(1));\\n            });\\n        }\\n    });\\n\\n    // or\\n    \\n    for _ in 0..10 {\\n        let pool = unsafe { py.new_pool() };\\n        let py = pool.python();\\n\\n        let a: Vec<u8> = vec![0; 40_000_000];\\n        let _bytes = PyBytes::new(py, &a);\\n\\n        std::thread::sleep(Duration::from_secs(1));\\n    }\\n\\n    Ok(())\\n}\\n```\\n\\n#### > Calling this function will consume 80MB of memory. :thumbsup: \\n\\n> [More info can be found here](https://pyo3.rs/main/memory.html#gil-bound-memory)\\n\\n> [Possible fix in Pyo3 0.21!](https://github.com/PyO3/pyo3/issues/3382)\\n>\\n\\n## Race condition\\n\\nLet\'s take another example, and imagine that we need to process data in different threads:\\n\\n```rust\\n/// Function GIL Lock\\n#[pyfunction]\\nfn gil_lock() {\\n    let start_time = Instant::now();\\n    std::thread::spawn(move || {\\n        Python::with_gil(|py| println!(\\"This threaded print was printed after {:#?}\\", &start_time.elapsed()));\\n    });\\n\\n    std::thread::sleep(Duration::from_secs(10));\\n}\\n```\\n\\n#### > This threaded print was printed after 10.0s. :cry: \\n\\nWhen using Python with `pyo3`, we have to make sure to know exactly when the GIL is locked or unlocked to avoid race conditions. \\n\\nIn the example above, the issue is that by default `pyo3` is going to lock the GIL in the main function thread, therefore blocking the spawned thread that is waiting for the GIL.\\n\\nIf we use the GIL in the main function thread or release the GIL in the main function thread, there is no issue.\\n\\n```rust\\n/// No gil lock\\n#[pyfunction]\\nfn gil_unlock() {\\n    let start_time = Instant::now();\\n    std::thread::spawn(move || {\\n        std::thread::sleep(Duration::from_secs(10));\\n    });\\n\\n    Python::with_gil(|py| println!(\\"1. This was printed after {:#?}\\", &start_time.elapsed()));\\n\\n    // or\\n\\n    let start_time = Instant::now();\\n    std::thread::spawn(move || {\\n        Python::with_gil(|py| println!(\\"2. This was printed after {:#?}\\", &start_time.elapsed()));\\n    });\\n    Python::with_gil(|py| {\\n        py.allow_threads(|| {\\n            std::thread::sleep(Duration::from_secs(10));\\n        })\\n    });\\n}\\n```\\n#### > \\"1\\" was printed after 32\xb5s and \\"2\\" was printed after 80\xb5s, so there was no race condition. :smile: \\n\\n## Tracing\\n\\nAs we can see, being able to measure the time spent when interfacing can be very valuable to identify bottlenecks. \\n\\nBut measuring the time spent manually as we did before can be tedious. \\n\\nWhat we can do is use a tracing library to do it for us. [Opentelemetry](https://opentelemetry.io/) can help us build a distributed observable system capable of bridging multiple languages. [Opentelemetry](https://opentelemetry.io/) can be used for tracing, metrics and logs.\\n\\nFor example, if we add:\\n```rust \\n/// No gil lock\\n#[pyfunction]\\nfn global_tracing(py: Python, func: Py<PyAny>) {\\n    // global::set_text_map_propagator(opentelemetry_jaeger::Propagator::new());\\n    global::set_text_map_propagator(TraceContextPropagator::new());\\n\\n    // Connect to Jaeger Opentelemetry endpoint\\n    // Start a new endpoint with:\\n    // docker run -d -p6831:6831/udp -p6832:6832/udp -p16686:16686 jaegertracing/all-in-one:latest\\n    let _tracer = opentelemetry_jaeger::new_agent_pipeline()\\n        .with_endpoint(\\"172.17.0.1:6831\\")\\n        .with_service_name(\\"rust_ffi\\")\\n        .install_simple()\\n        .unwrap();\\n\\n    let tracer = global::tracer(\\"test\\");\\n\\n    // Parent Trace, first trace\\n    let _ = tracer.in_span(\\"parent_python_work\\", |cx| -> Result<()> { \\n        std::thread::sleep(Duration::from_secs(1));\\n        \\n        let mut map = HashMap::new();\\n        global::get_text_map_propagator(|propagator| propagator.inject_context(&cx, &mut map));\\n\\n        let output = func\\n            .call1(py, (map,))\\n            .map_err(traceback)\\n            .context(\\"function called failed\\")?;\\n        let out_map: HashMap<String, String> = output.extract(py).unwrap();\\n        let out_context = global::get_text_map_propagator(|prop| prop.extract(&out_map));\\n\\n        std::thread::sleep(Duration::from_secs(1));\\n\\n        let _span = tracer.start_with_context(\\"after_python_work\\", &out_context); // third trace\\n\\n        Ok(())\\n    });\\n}\\n```\\n\\n\\nAnd the following, in the Python code:\\n```python\\ndef abc(cx):\\n    propagator = TraceContextTextMapPropagator()\\n    context = propagator.extract(carrier=cx)\\n\\n    with tracing.tracer.start_as_current_span(\\n        name=\\"Python_span\\", context=context\\n    ) as child_span:\\n        child_span.add_event(\\"in Python!\\")\\n        output = {}\\n        tracing.propagator.inject(output)\\n        time.sleep(2)\\n    return output\\n```\\n\\nWe will get the following traces:\\n\\n![](/img/blogpost_ffi.png)\\n\\nUsing this we can measure the time spent when interfacing languages, identify lock issues, and with the combination of logs and metrics, reduce the complexity of multi-language libraries.\\n\\n# [dora-rs](https://github.com/dora-rs/dora)\\n\\nHopefully, this small blog post should help you identify FFI issues. \\n\\nAll optimization above have already been implemented within [dora-rs](https://github.com/dora-rs/dora) that lets you build fast and simple dataflows using Rust, Python, C and C++.\\n\\nYou\'re very welcome to check out [dora-rs](https://github.com/dora-rs/dora) if bridging languages in a dataflow is your usecase.\\n\\nWe just recently opened a Discord and you can reach out there for literally any question, even just for a quick chat: https://discord.gg/DXJ6edAtym\\n\\nI\'m also going to present this FFI work at [GOSIM Workshop in Shanghai on the 23rd of Sept 2023](https://workshop2023.gosim.org/schedule#auto)!\\n\\nFor more info on `dora-rs`:\\n- Github: https://github.com/dora-rs/dora\\n- Website: https://www.dora-rs.ai/\\n- Discord: https://discord.gg/XqhQaN8P"}]}}')}}]);