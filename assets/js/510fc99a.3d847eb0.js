"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[1657],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>d});var o=t(96540);const a={},r=o.createContext(a);function i(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(r.Provider,{value:n},e.children)}},52515:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>s});const o=JSON.parse('{"id":"guides/Development/Cuda","title":"CUDA 0-Copy IPC","description":"So let say you have a pytorch tensor on cuda and you want to share it between nodes.","source":"@site/docs/guides/Development/Cuda.md","sourceDirName":"guides/Development","slug":"/guides/Development/Cuda","permalink":"/docs/guides/Development/Cuda","draft":false,"unlisted":false,"editUrl":"https://github.com/dora-rs/dora-rs.github.io/edit/main/docs/guides/Development/Cuda.md","tags":[],"version":"current","frontMatter":{},"sidebar":"guides","previous":{"title":"Arrow","permalink":"/docs/guides/Development/Arrow"},"next":{"title":"Dynamic Node","permalink":"/docs/guides/Development/dynamic-node"}}');var a=t(74848),r=t(28453);const i={},d="CUDA 0-Copy IPC",c={},s=[{value:"Installation",id:"installation",level:2},{value:"Sending data",id:"sending-data",level:2},{value:"Receiving data",id:"receiving-data",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"cuda-0-copy-ipc",children:"CUDA 0-Copy IPC"})}),"\n",(0,a.jsx)(n.p,{children:"So let say you have a pytorch tensor on cuda and you want to share it between nodes."}),"\n",(0,a.jsx)(n.p,{children:"Good news is that you can do it without copying the data using CUDA 0 Copy IPC."}),"\n",(0,a.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(n.p,{children:"To use this feature, make sure to have the following requirements:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'\n# Install pyarrow with gpu support\nconda install pyarrow "arrow-cpp-proc=*=cuda" -c conda-forge\n\n## Test installation with\npython -c "import pyarrow.cuda"\n\n# Install numba for translation from arrow to torch\npip install numba\n\n## Test installation with\npython -c "import numba.cuda"\n\n# Install torch if it\'s not already present\npip install torch\n\n## Test installation with\npython -c "import torch; assert torch.cuda.is_available()"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sending-data",children:"Sending data"}),"\n",(0,a.jsx)(n.p,{children:"To create an IPC handle that is going to be sent between process, do the following:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nfrom dora.cuda import torch_to_ipc_buffer\n\ntorch_tensor = torch.tensor([1, 2, 3], dtype=torch.int64, device="cuda")\nipc_buffer, metadata = torch_to_ipc_buffer(torch_tensor)\nnode.send_output("latency", ipc_buffer, metadata)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"receiving-data",children:"Receiving data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import pyarrow as pa\nfrom dora import Node\nfrom dora.cuda import ipc_buffer_to_ipc_handle, cudabuffer_to_torch\n\nctx = pa.cuda.context()\nnode = Node()\nevent = node.next() # Get an event with a torch handle\n\nipc_handle = ipc_buffer_to_ipc_handle(event["value"])\ncudabuffer = ctx.open_ipc_buffer(ipc_handle)\ntorch_tensor = cudabuffer_to_torch(cudabuffer, event["metadata"])  # on cuda\n'})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);