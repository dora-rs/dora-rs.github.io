"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[8732],{85217:(n,o,e)=>{e.r(o),e.d(o,{assets:()=>r,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var t=e(74848),i=e(28453);const a={},s="Obstacle location operator",l={id:"nodes_operators/obstacle_location_op",title:"Obstacle location operator",description:"The obstacle location operator match bounding box with depth frame to find an approximative position of obstacles.",source:"@site/docs/nodes_operators/obstacle_location_op.md",sourceDirName:"nodes_operators",slug:"/nodes_operators/obstacle_location_op",permalink:"/docs/nodes_operators/obstacle_location_op",draft:!1,unlisted:!1,editUrl:"https://github.com/dora-rs/dora-rs.github.io/edit/main/docs/nodes_operators/obstacle_location_op.md",tags:[],version:"current",frontMatter:{},sidebar:"nodes_operators",previous:{title:"MiDaS",permalink:"/docs/nodes_operators/midas_op"},next:{title:"PID Control operator",permalink:"/docs/nodes_operators/pid_control_op"}},r={},c=[{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Example plot (green dor in the middle of the bounding box)",id:"example-plot-green-dor-in-the-middle-of-the-bounding-box",level:2},{value:"Graph Description",id:"graph-description",level:2},{value:"Graph Viz",id:"graph-viz",level:2},{value:"Methods",id:"methods",level:2},{value:"<code>__init__()</code>",id:"__init__",level:3},{value:"<code>.on_event(...)</code>",id:"on_event",level:3},{value:"<code>.on_input(...)</code>",id:"on_input",level:3}];function d(n){const o={code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",mermaid:"mermaid",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...n.components},{Details:e}=o;return e||function(n,o){throw new Error("Expected "+(o?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.h1,{id:"obstacle-location-operator",children:"Obstacle location operator"}),"\n",(0,t.jsx)(o.p,{children:"The obstacle location operator match bounding box with depth frame to find an approximative position of obstacles."}),"\n",(0,t.jsx)(o.p,{children:"There is two logic within it:"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"One is for the ground dot for lane detection."}),"\n",(0,t.jsx)(o.li,{children:"One is for bounding box obstacle localisation."}),"\n"]}),"\n",(0,t.jsx)(o.p,{children:"Both logic are based on he computation of the projection in 2D space of the lidar 3D point and then reusing the index to get the 3D position."}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"In the case of ground dot detection, the approximation is based on a knnr, as we might not have enough data on the floor."}),"\n",(0,t.jsx)(o.li,{children:"In the case of bounding box, we use first quantile closest point within the bounding box to estimate the distance. We use the first quantile closest point to remove the noise."}),"\n"]}),"\n",(0,t.jsxs)(o.p,{children:["The mecanism to project the lidar point cloud into a 2D is also used in the ",(0,t.jsx)(o.code,{children:"plot.py"})," operator. You can use the input ",(0,t.jsx)(o.code,{children:"lidar_pc"})," within it to help you debug."]}),"\n",(0,t.jsx)(o.h2,{id:"inputs",children:"Inputs"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"2D Obstacles bounding box."}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"outputs",children:"Outputs"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"3D position of obstacles as dot."}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"example-plot-green-dor-in-the-middle-of-the-bounding-box",children:"Example plot (green dor in the middle of the bounding box)"}),"\n",(0,t.jsx)(o.p,{children:(0,t.jsx)(o.img,{src:"https://i.imgur.com/Aq33qy5.png",alt:"Imgur"})}),"\n",(0,t.jsx)(o.h2,{id:"graph-description",children:"Graph Description"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-yaml",children:"  - id: obstacle_location_op\n    operator: \n      outputs:\n        - obstacles\n      inputs:\n        lidar_pc: oasis_agent/lidar_pc\n        obstacles_bbox: yolov5/bbox\n        position: oasis_agent/position\n      python: ../../operators/obstacle_location_op.py\n"})}),"\n",(0,t.jsx)(o.h2,{id:"graph-viz",children:"Graph Viz"}),"\n",(0,t.jsx)(o.mermaid,{value:"        flowchart TB\n  oasis_agent\nsubgraph yolov5\n  yolov5/op[op]\nend\nsubgraph fot_op\n  fot_op/op[op]\nend\nsubgraph obstacle_location_op\n  obstacle_location_op/op[op]\nend\n  oasis_agent -- lidar_pc --\x3e obstacle_location_op/op\n  yolov5/op -- bbox as obstacles_bbox --\x3e obstacle_location_op/op\n  oasis_agent -- position --\x3e obstacle_location_op/op\n  obstacle_location_op/op -- obstacles --\x3e fot_op/op"}),"\n",(0,t.jsx)(o.h2,{id:"methods",children:"Methods"}),"\n",(0,t.jsx)(o.h3,{id:"__init__",children:(0,t.jsx)(o.code,{children:"__init__()"})}),"\n",(0,t.jsxs)(e,{children:[(0,t.jsx)("summary",{children:"Source Code"}),(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:"    def __init__(self):\n        self.point_cloud = []\n        self.camera_point_cloud = []\n        self.ground_point_cloud = []\n        self.camera_ground_point_cloud = []\n        self.last_point_cloud = []\n        self.last_camera_point_cloud = []\n        self.obstacles = []\n        self.obstacles_bbox = []\n        self.position = []\n        self.lanes = []\n\n\n"})})]}),"\n",(0,t.jsx)(o.h3,{id:"on_event",children:(0,t.jsx)(o.code,{children:".on_event(...)"})}),"\n",(0,t.jsxs)(e,{children:[(0,t.jsx)("summary",{children:"Source Code"}),(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'\n    def on_event(\n        self,\n        dora_event: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        if dora_event["type"] == "INPUT":\n            return self.on_input(dora_event, send_output)\n        return DoraStatus.CONTINUE\n\n\n'})})]}),"\n",(0,t.jsx)(o.h3,{id:"on_input",children:(0,t.jsx)(o.code,{children:".on_input(...)"})}),"\n",(0,t.jsxs)(e,{children:[(0,t.jsx)("summary",{children:"Source Code"}),(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'\n    def on_input(\n        self,\n        dora_input: dict,\n        send_output: Callable[[str, bytes], None],\n    ):\n        if "lidar_pc" == dora_input["id"]:\n            point_cloud = np.array(dora_input["value"])\n            point_cloud = point_cloud.reshape((-1, 3))\n\n            # From Velodyne axis to Camera axis\n            # from Velodyne axis:\n            # x -> forward, y -> right, z -> top\n            # to Camera axis:\n            # x -> right, y -> bottom, z -> forward\n            point_cloud = np.dot(\n                point_cloud,\n                VELODYNE_MATRIX,\n            )\n\n            # Forward points only ( forward = z > 0.1 )\n            point_cloud = point_cloud[np.where(point_cloud[:, 2] > 0.1)]\n\n            # Remove ground points. Above lidar only ( bottom = y < 1.0 )\n            above_ground_point_index = np.where(point_cloud[:, 1] < 1.0)\n            point_cloud = point_cloud[above_ground_point_index]\n            self.ground_point_cloud = point_cloud[above_ground_point_index == False]\n\n            # 3D array -> 2D array with index_x -> pixel x, index_y -> pixel_y, value -> z\n            camera_point_cloud = local_points_to_camera_view(\n                point_cloud, INTRINSIC_MATRIX\n            ).T\n            self.camera_ground_point_cloud = local_points_to_camera_view(\n                self.ground_point_cloud, INTRINSIC_MATRIX\n            ).T\n\n            self.camera_point_cloud = camera_point_cloud\n            self.point_cloud = point_cloud\n\n        elif "position" == dora_input["id"]:\n            # Add sensor transform\n            self.position = dora_input["value"].to_numpy()\n            self.extrinsic_matrix = get_extrinsic_matrix(\n                get_projection_matrix(self.position)\n            )\n\n        elif "lanes" == dora_input["id"]:\n            lanes = np.array(dora_input["value"]).reshape((-1, 60, 2))\n\n            knnr = KNeighborsRegressor(n_neighbors=4)\n            knnr.fit(self.camera_ground_point_cloud[:, :2], self.ground_point_cloud)\n\n            processed_lanes = []\n            for lane in lanes:\n                lane_location = knnr.predict(lane)\n                lane_location = np.array(lane_location)\n\n                lane_location = np.hstack(\n                    (\n                        lane_location,\n                        np.ones((lane_location.shape[0], 1)),\n                    )\n                )\n                lane_location = np.dot(lane_location, self.extrinsic_matrix.T)[:, :3]\n                processed_lanes.append(lane_location)\n            processed_lanes = pa.array(np.array(processed_lanes, np.float32).ravel())\n\n            send_output("global_lanes", processed_lanes, dora_input["metadata"])\n\n        elif "obstacles_bbox" == dora_input["id"]:\n            if len(self.position) == 0 or len(self.point_cloud) == 0:\n                return DoraStatus.CONTINUE\n\n            # bbox = np.array([[min_x, max_x, min_y, max_y, confidence, label], ... n_bbox ... ])\n            self.obstacles_bbox = np.array(dora_input["value"]).reshape((-1, 6))\n\n            obstacles_with_location = []\n            for obstacle_bb in self.obstacles_bbox:\n                [min_x, max_x, min_y, max_y, confidence, label] = obstacle_bb\n                z_points = self.point_cloud[\n                    np.where(\n                        (self.camera_point_cloud[:, 0] > min_x)\n                        & (self.camera_point_cloud[:, 0] < max_x)\n                        & (self.camera_point_cloud[:, 1] > min_y)\n                        & (self.camera_point_cloud[:, 1] < max_y)\n                    )\n                ]\n                if len(z_points) > 0:\n                    closest_point = z_points[\n                        z_points[:, 2].argsort()[int(len(z_points) / 4)]\n                    ]\n                    obstacles_with_location.append(closest_point)\n            if len(obstacles_with_location) > 0:\n                obstacles_with_location = np.array(obstacles_with_location)\n                obstacles_with_location = np.hstack(\n                    (\n                        obstacles_with_location,\n                        np.ones((obstacles_with_location.shape[0], 1)),\n                    )\n                )\n                obstacles_with_location = np.dot(\n                    obstacles_with_location, self.extrinsic_matrix.T\n                )[:, :3]\n\n                predictions = get_predictions(\n                    self.obstacles_bbox, obstacles_with_location\n                )\n                predictions_bytes = pa.array(np.array(predictions, np.float32).ravel())\n\n                send_output("obstacles", predictions_bytes, dora_input["metadata"])\n            else:\n                send_output(\n                    "obstacles",\n                    pa.array(np.array([]).ravel()),\n                    dora_input["metadata"],\n                )\n        return DoraStatus.CONTINUE\n\n\n'})})]})]})}function p(n={}){const{wrapper:o}={...(0,i.R)(),...n.components};return o?(0,t.jsx)(o,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},28453:(n,o,e)=>{e.d(o,{R:()=>s,x:()=>l});var t=e(96540);const i={},a=t.createContext(i);function s(n){const o=t.useContext(a);return t.useMemo((function(){return"function"==typeof n?n(o):{...o,...n}}),[o,n])}function l(n){let o;return o=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(a.Provider,{value:o},n.children)}}}]);