"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[1609],{53088:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>l,frontMatter:()=>t,metadata:()=>s,toc:()=>d});var r=a(74848),i=a(28453);const t={authors:"haixuan",title:"Reachy2 Speech-to-Grasp",description:"Using reachy2 with QwenVL2 and SAM2 in order to create a robust universal* grasper. *We currently does not optimise for grasp pose, but it is going to come"},o=void 0,s={permalink:"/zh-CN/blog/reachy-qwenvl-sam2",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/reachy-qwenvl-sam2.md",source:"@site/blog/reachy-qwenvl-sam2.md",title:"Reachy2 Speech-to-Grasp",description:"Using reachy2 with QwenVL2 and SAM2 in order to create a robust universal* grasper. *We currently does not optimise for grasp pose, but it is going to come",date:"2025-04-17T21:03:27.000Z",tags:[],readingTime:4.035,hasTruncateMarker:!1,authors:[{name:"Haixuan Xavier Tao",title:"Maintainer of dora-rs",url:"https://github.com/haixuantao",imageURL:"https://github.com/haixuantao.png",key:"haixuan"}],frontMatter:{authors:"haixuan",title:"Reachy2 Speech-to-Grasp",description:"Using reachy2 with QwenVL2 and SAM2 in order to create a robust universal* grasper. *We currently does not optimise for grasp pose, but it is going to come"},unlisted:!1,prevItem:{title:"Reachy2 Pick and Place",permalink:"/zh-CN/blog/reachy-pick-place"},nextItem:{title:"Rust-Python FFI",permalink:"/zh-CN/blog/rust-python"}},c={authorsImageUrls:[void 0]},d=[{value:"Grasping limitation",id:"grasping-limitation",level:2},{value:"Main ideas",id:"main-ideas",level:2},{value:"Demo features that are technically not necessary",id:"demo-features-that-are-technically-not-necessary",level:2},{value:"Source Code",id:"source-code",level:2},{value:"Computer requirements",id:"computer-requirements",level:2},{value:"Annex: Rerun",id:"annex-rerun",level:2},{value:"Annex: Graph of nodes running in parallel",id:"annex-graph-of-nodes-running-in-parallel",level:2},{value:"Annex: Video",id:"annex-video",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",hr:"hr",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Reachy grasping demo showcase how by combining multiple AI models together we can create a robot able to grasp object on a table from our speech autonomously."}),"\n",(0,r.jsx)(n.p,{children:"This is performed with 100% open source code."}),"\n",(0,r.jsx)(n.p,{children:"This approach also has the advantage of not be dependent on any environemnt finetuning meaning it can work pretty much anywhere with any robot out of the box."}),"\n",(0,r.jsxs)(n.admonition,{type:"warning",children:[(0,r.jsx)(n.h2,{id:"grasping-limitation",children:"Grasping limitation"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Grasping is limited to small concave object that fits in reachy's claws."}),"\n",(0,r.jsx)(n.li,{children:"Grasping always has a fixed rotation angle pose."}),"\n",(0,r.jsx)(n.li,{children:"Current trajectory are predetermined."}),"\n"]}),(0,r.jsx)(n.p,{children:"By universal grasping, we want to focus on the fact that the object can be any object as long as we can define it using a generalistic prompt as opposed to previous approach dependant on predefined label."})]}),"\n",(0,r.jsx)(n.h2,{id:"main-ideas",children:"Main ideas"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Convert audio to a sequence using ",(0,r.jsx)(n.strong,{children:"Silero VAD"})]}),"\n",(0,r.jsxs)(n.li,{children:["Convert the sequence to text using ",(0,r.jsx)(n.strong,{children:"OpenAI Whisper"})]}),"\n",(0,r.jsxs)(n.li,{children:["Convert the user's text and the rgb image from ",(0,r.jsx)(n.strong,{children:"Orbecc Gemini 336 Camera (camera torso)"})," into a bounding box using ",(0,r.jsx)(n.strong,{children:"QwenVL 2.5"})]}),"\n",(0,r.jsxs)(n.li,{children:["Convert the bounding box into a masks using ",(0,r.jsx)(n.strong,{children:"Meta SAM2"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Convert the masks and the depth image of ",(0,r.jsx)(n.strong,{children:"Orbecc Gemini 336 Camera"})," into a position using efficient ",(0,r.jsx)(n.strong,{children:"rust"})," code powered by ",(0,r.jsx)(n.strong,{children:"dora-rs."})]}),"\n",(0,r.jsxs)(n.li,{children:["Go to the position using inverse kinematics provided by ",(0,r.jsx)(n.strong,{children:"Pollen Robotics"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Go to predetermined position and come back in a scripted way. (In the future we plan to automate this. )"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"demo-features-that-are-technically-not-necessary",children:"Demo features that are technically not necessary"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"We have added head movement by searching for people when idle."}),"\n",(0,r.jsx)(n.li,{children:"We have made the head look at the predicted bounding box when the object is identified."}),"\n",(0,r.jsx)(n.li,{children:"We have made the robot turn left/right and releasing object into the box in a predetermined way in order to make people imagine follow up possibility."}),"\n",(0,r.jsx)(n.li,{children:"We have used both arms of the robot also using only one would also work, although, the grasping area given a fixed mobile base would be greatly reduced."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"source-code",children:"Source Code"}),"\n",(0,r.jsxs)(n.p,{children:["All the source code and instructions are contained within dora-rs repository: ",(0,r.jsx)(n.a,{href:"https://github.com/dora-rs/dora/pull/784",children:"https://github.com/dora-rs/dora/pull/784"})]}),"\n",(0,r.jsxs)(n.p,{children:["The code is 100% open source and is aimed at being 100% reusable on other hardware using dora-rs by just replacing: ",(0,r.jsx)(n.code,{children:"reachy-left-arm, reachy-right-arm, reachy-camera and reachy-head"}),". Albeit, porting this code might not be 100% easy as of now, and further work need to be done."]}),"\n",(0,r.jsx)(n.h2,{id:"computer-requirements",children:"Computer requirements"}),"\n",(0,r.jsx)(n.p,{children:"This run on both:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"MacOS with ~ 20G of RAM but without SAM2 as SAM2 is only available on nvidia for now."}),"\n",(0,r.jsx)(n.li,{children:"Linux with ~ 10G of Nvidia VRAM and ~16G of Nvidia VRAM using SAM2."}),"\n",(0,r.jsx)(n.li,{children:"I have not tried Windows but should run as well."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"annex-rerun",children:"Annex: Rerun"}),"\n",(0,r.jsx)("iframe",{src:"https://app.rerun.io/version/0.21.0/index.html?url=https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/final_data.rrd",width:"100%",height:"700px"}),"\n",(0,r.jsx)(n.p,{children:"In the above iframe, the important information are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/text_whisper"}),": correspond to ",(0,r.jsx)(n.strong,{children:"whisper"})," audio transcription."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/text_response"}),": correspond to the bounding box given as plain text from ",(0,r.jsx)(n.strong,{children:"QwenVL 2.5"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"camera_torso"}),": correspond to ",(0,r.jsx)(n.strong,{children:"Orbecc Gemini 336 Depth Camera"})," rgb image."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"camera_torso"})," bounding box: correspond to the QwenVL bounding box projected on the image that is going to be used to grasp object. The prediction is done at regular interval and does not disappear. Sorry if it can be a bit confusing."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"camera_left"})," bounding box: correspond to the QwenVL bounding box for detecting humans for head movement and is strictly a gimmick feature."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"annex-graph-of-nodes-running-in-parallel",children:"Annex: Graph of nodes running in parallel"}),"\n",(0,r.jsx)(n.mermaid,{value:'    flowchart TB\n    dora-microphone["**dora-microphone**"]\n    sam2["**sam2**"]\n    dora-vad["**dora-vad**"]\n    dora-distil-whisper["**dora-distil-whisper**"]\n    reachy-mobile-base["**reachy-mobile-base**"]\n    reachy-left-arm["**reachy-left-arm**"]\n    reachy-right-arm["**reachy-right-arm**"]\n    reachy-camera["**reachy-camera**"]\n    reachy-head[/"**reachy-head**"\\]\n    plot[/"**plot**"\\]\n    dora-qwenvl["**dora-qwenvl**"]\n    parse_bbox["**parse_bbox**"]\n    box_coordinates["**box_coordinates**"]\n\n    state_machine["**state_machine**"]\n    subgraph ___dora___ [dora]\n    subgraph ___timer_timer___ [timer]\n    dora/timer/millis/50[\\millis/50/]\n    dora/timer/millis/500[\\millis/500/]\n    dora/timer/secs/2[\\secs/2/]\n    end\n    end\n    dora/timer/secs/2 -- tick --\x3e dora-microphone\n    parse_bbox -- bbox as boxes2d --\x3e sam2\n    reachy-camera -- image_depth --\x3e sam2\n    dora-microphone -- audio --\x3e dora-vad\n    dora-vad -- audio as input --\x3e dora-distil-whisper\n    state_machine -- action_base --\x3e reachy-mobile-base\n    state_machine -- action_l_arm as pose --\x3e reachy-left-arm\n    state_machine -- action_r_arm as pose --\x3e reachy-right-arm\n    dora/timer/millis/50 -- tick --\x3e reachy-camera\n    parse_bbox -- bbox_face as boxes2d --\x3e reachy-head\n    state_machine -- look --\x3e reachy-head\n    parse_bbox -- bbox_face as camera_left/boxes2d_face --\x3e plot\n    reachy-camera -- image_right as camera_left/image_right --\x3e plot\n    parse_bbox -- bbox as camera_torso/boxes2d --\x3e plot\n    reachy-camera -- image_depth as camera_torso/image --\x3e plot\n    dora-qwenvl -- text as text_response --\x3e plot\n    dora-distil-whisper -- text as text_whisper --\x3e plot\n    reachy-camera -- image_depth --\x3e dora-qwenvl\n    reachy-camera -- image_left --\x3e dora-qwenvl\n    dora/timer/millis/500 -- text_1 --\x3e dora-qwenvl\n    state_machine -- text_vlm as text_2 --\x3e dora-qwenvl\n    dora-qwenvl -- text --\x3e parse_bbox\n    reachy-camera -- depth --\x3e box_coordinates\n    sam2 -- masks --\x3e box_coordinates\n    box_coordinates -- pose --\x3e state_machine\n    reachy-mobile-base -- response_base --\x3e state_machine\n    reachy-left-arm -- response_l_arm --\x3e state_machine\n    reachy-right-arm -- response_r_arm --\x3e state_machine\n    dora-distil-whisper -- text --\x3e state_machine'}),"\n",(0,r.jsx)(n.p,{children:"The above graph correspond to the exact communication channels between the nodes that are running concurrently."}),"\n",(0,r.jsx)(n.h2,{id:"annex-video",children:"Annex: Video"}),"\n",(0,r.jsx)("video",{controls:!0,src:"https://huggingface.co/datasets/haixuantao/rerun_dataset/resolve/main/761752120155.mp4",width:"100%"}),"\n",(0,r.jsxs)(n.p,{children:["In case you want to hire or buy reachy, you can send him an email at ",(0,r.jsx)(n.a,{href:"mailto:reachy@1ms.ai",children:"reachy@1ms.ai"})]})]})}function l(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>s});var r=a(96540);const i={},t=r.createContext(i);function o(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);