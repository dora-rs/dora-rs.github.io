"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[6922],{85575:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var i=t(74848),s=t(28453);const o={},r="MiDaS (Mixed Frequency Data Sampling Regression Models, \u6df7\u9891\u6570\u636e\u91c7\u6837\u56de\u5f52\u6a21\u578b)",a={id:"nodes_operators/midas_op",title:"MiDaS (Mixed Frequency Data Sampling Regression Models, \u6df7\u9891\u6570\u636e\u91c7\u6837\u56de\u5f52\u6a21\u578b)",description:"\u7528\u4e8e\u4ece\u5355\u4e2a\u56fe\u50cf\u8ba1\u7b97\u76f8\u5bf9\u6df1\u5ea6\u7684 MiDaS \u6a21\u578b (Mixed Frequency Data Sampling Regression Models, \u6df7\u9891\u6570\u636e\u91c7\u6837\u56de\u5f52\u6a21\u578b) \u3002",source:"@site/i18n/zh-CN/docusaurus-plugin-content-docs/current/nodes_operators/midas_op.md",sourceDirName:"nodes_operators",slug:"/nodes_operators/midas_op",permalink:"/zh-CN/docs/nodes_operators/midas_op",draft:!1,unlisted:!1,editUrl:"https://crowdin.com/dora-rs/zh-CN",tags:[],version:"current",frontMatter:{},sidebar:"nodes_operators",previous:{title:"FOT( Frenet Optimal Planner, Frenet \u6700\u4f18\u89c4\u5212\u5e08) \u7b97\u5b50",permalink:"/zh-CN/docs/nodes_operators/fot_op"},next:{title:"Obstacle location (\u969c\u788d\u7269\u4f4d\u7f6e) \u7b97\u5b50",permalink:"/zh-CN/docs/nodes_operators/obstacle_location_op"}},d={},l=[{value:"\u5b89\u88c5\uff1a",id:"\u5b89\u88c5",level:3},{value:"\u8f93\u5165",id:"\u8f93\u5165",level:2},{value:"\u8f93\u51fa",id:"\u8f93\u51fa",level:2},{value:"\u793a\u4f8b\u8f93\u51fa",id:"\u793a\u4f8b\u8f93\u51fa",level:2},{value:"\u65b9\u6cd5",id:"\u65b9\u6cd5",level:2},{value:"<code>__init__()</code>",id:"__init__",level:3},{value:"<code>.on_event(...)</code>",id:"on_event",level:3},{value:"<code>.on_input(...)</code>",id:"on_input",level:3}];function c(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"midas-mixed-frequency-data-sampling-regression-models-\u6df7\u9891\u6570\u636e\u91c7\u6837\u56de\u5f52\u6a21\u578b",children:"MiDaS (Mixed Frequency Data Sampling Regression Models, \u6df7\u9891\u6570\u636e\u91c7\u6837\u56de\u5f52\u6a21\u578b)"}),"\n",(0,i.jsx)(n.p,{children:"\u7528\u4e8e\u4ece\u5355\u4e2a\u56fe\u50cf\u8ba1\u7b97\u76f8\u5bf9\u6df1\u5ea6\u7684 MiDaS \u6a21\u578b (Mixed Frequency Data Sampling Regression Models, \u6df7\u9891\u6570\u636e\u91c7\u6837\u56de\u5f52\u6a21\u578b) \u3002"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"MiDaS \u4ece\u5355\u4e2a\u56fe\u50cf\u8ba1\u7b97\u76f8\u5bf9\u53cd\u5411\u6df1\u5ea6\u3002 \u8be5\u5b58\u50a8\u5e93\u63d0\u4f9b\u4e86\u591a\u4e2a\u6a21\u578b\uff0c\u6db5\u76d6\u4e0d\u540c\u7684\u7528\u4f8b\uff0c\u4ece\u5c0f\u578b\u9ad8\u901f\u6a21\u578b\u5230\u63d0\u4f9b\u6700\u9ad8\u7cbe\u5ea6\u7684\u8d85\u5927\u578b\u6a21\u578b\u3002 \u8fd9\u4e9b\u6a21\u578b\u5df2\u4f7f\u7528\u591a\u76ee\u6807\u4f18\u5316\u5728 10 \u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u4ee5\u786e\u4fdd\u5728\u5404\u79cd\u8f93\u5165\u4e0a\u83b7\u5f97\u9ad8\u8d28\u91cf\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u5b89\u88c5",children:"\u5b89\u88c5\uff1a"}),"\n",(0,i.jsx)(n.p,{children:"midas \u79bb\u7ebf\u5b89\u88c5\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd $DORA_DEP_HOME/dependencies/\ngit clone git@github.com:isl-org/MiDaS.git\ncd MiDaS/weights\n# \u5982\u679c\u4e0d\u60f3\u6dfb\u52a0\u624b\u52a8\u4e0b\u8f7d\uff0c\u7a0b\u5e8f\u5c06\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\nwget https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt\ncp midas_v21_small_256.pt $HOME/.cache/torch/hub/checkpoints/\n"})}),"\n",(0,i.jsx)(n.h2,{id:"\u8f93\u5165",children:"\u8f93\u5165"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u56fe\u50cf: \u9ad8 x \u5bbd x BGR array."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u8f93\u51fa",children:"\u8f93\u51fa"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"bbox: \u9ad8 x \u5bbd x \u76f8\u5bf9\u6df1\u5ea6\u6570\u7ec4\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u793a\u4f8b\u8f93\u51fa",children:"\u793a\u4f8b\u8f93\u51fa"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/UrF9iPN.png",alt:"Imgur"})}),"\n",(0,i.jsx)(n.p,{children:"\u6dfb\u52a0\u4ee5\u4e0b\u6570\u636e\u6d41\u914d\u7f6e"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'  - id: midas_op\n    operator:\n      outputs:\n        - depth_frame\n      inputs:\n        image: webcam/image\n      python: ../../operators/midas_op.py\n    env:\n      PYTORCH_DEVICE: "cuda"\n      MIDAS_PATH: $DORA_DEP_HOME/dependencies/MiDaS/\n      MIDAS_WEIGHT_PATH: $DORA_DEP_HOME/dependencies/MiDaS/weights/midas_v21_small_256.pt\n      MODEL_TYPE: "MiDaS_small"\n      MODEL_NAME: "MiDaS_small"\n'})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'model_type = "DPT_Large"     # MiDaS v3 - Large     (\u6700\u9ad8\u7cbe\u5ea6\uff0c\u6700\u6162\u7684\u63a8\u7406\u901f\u5ea6)'}),"\n",(0,i.jsx)(n.li,{children:'model_type = "DPT_Hybrid"   # MiDaS v3 - Hybrid    (\u4e2d\u7b49\u7cbe\u5ea6\uff0c\u4e2d\u7b49\u63a8\u7406\u901f\u5ea6)'}),"\n",(0,i.jsx)(n.li,{children:'model_type = "MiDaS_small"  # MiDaS v2.1 - Small   (\u6700\u4f4e\u7cbe\u5ea6\uff0c\u6700\u9ad8\u63a8\u7406\u901f\u5ea6)'}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u65b9\u6cd5",children:"\u65b9\u6cd5"}),"\n",(0,i.jsx)(n.h3,{id:"__init__",children:(0,i.jsx)(n.code,{children:"__init__()"})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"\u6e90\u7801"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'    def __init__(self):\n        if MIDAS_PATH is None:\n            # \u53ef\u80fd\u9700\u8981\u4e92\u8054\u7f51\n            self.model = torch.hub.load(\n                "intel-isl/MiDaS",\n                MODEL_TYPE,\n            )\n            midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")\n        else:\n            # \u5927\u6982\u4e0d\u9700\u8981\u4e92\u8054\u7f51\n            self.model = torch.hub.load(\n                repo_or_dir=MIDAS_PATH,\n                model=MODEL_NAME,\n                weights=MIDAS_WEIGHT_PATH,\n                source="local",\n            )\n            midas_transforms = torch.hub.load(\n                repo_or_dir=MIDAS_PATH, model="transforms", source="local"\n            )\n        if MODEL_TYPE == "DPT_Large" or MODEL_TYPE == "DPT_Hybrid":\n            self.transform = midas_transforms.dpt_transform\n        else:\n            self.transform = midas_transforms.small_transform\n        self.model.to(torch.device(DEVICE))\n        self.model.eval()\n\n\n'})})]}),"\n",(0,i.jsx)(n.h3,{id:"on_event",children:(0,i.jsx)(n.code,{children:".on_event(...)"})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"\u6e90\u7801"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\n    def on_event(\n        self,\n        dora_event: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        if dora_event["type"] == "INPUT":\n            return self.on_input(dora_event, send_output)\n        return DoraStatus.CONTINUE\n\n\n'})})]}),"\n",(0,i.jsx)(n.h3,{id:"on_input",children:(0,i.jsx)(n.code,{children:".on_input(...)"})}),"\n",(0,i.jsx)(n.p,{children:'\u56fe\u50cf\u53e5\u67c4 \u53c2\u6570: dora_input["id"]  (str): yaml \u914d\u7f6e\u4e2d\u58f0\u660e\u7684\u8f93\u5165\u7684Id dora_input["data"] (bytes): \u5b57\u8282\u5f62\u5f0f\u7684\u8f93\u5165\u6d88\u606f send_output (Callable[[str, bytes]]): \u51fd\u6570\u4f7f\u8f93\u51fa\u53d1\u9001\u56dedora\u3002'}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"\u6e90\u7801"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\n    def on_input(\n        self,\n        dora_input: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        """\u56fe\u50cf\u53e5\u67c4\n        \u53c2\u6570:\n            dora_input["id"]  (str): yaml \u914d\u7f6e\u4e2d\u58f0\u660e\u7684\u8f93\u5165\u7684Id\n            dora_input["data"] (bytes): \u5b57\u8282\u5f62\u5f0f\u7684\u8f93\u5165\u6d88\u606f\n            send_output (Callable[[str, bytes]]): \u51fd\u6570\u4f7f\u8f93\u51fa\u53d1\u9001\u56dedora\u3002\n        """\n        if dora_input["id"] == "image":\n            # \u8f6c\u6362 bytes \u7c7b\u578b \u81f3 numpy array \u7c7b\u578b\n            frame = np.frombuffer(\n                dora_input["data"],\n                np.uint8,\n            ).reshape((IMAGE_HEIGHT, IMAGE_WIDTH, 4))\n\n            with torch.no_grad():\n                image = frame[:, :, :3]\n                img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                input_batch = self.transform(img).to(DEVICE)\n                prediction = self.model(input_batch)\n                prediction = torch.nn.functional.interpolate(\n                    prediction.unsqueeze(1),\n                    size=img.shape[:2],\n                    mode="bicubic",\n                    align_corners=False,\n                ).squeeze()\n                depth_output = prediction.cpu().numpy()\n                depth_min = depth_output.min()\n                depth_max = depth_output.max()\n                normalized_depth = (\n                    255 * (depth_output - depth_min) / (depth_max - depth_min)\n                )\n                normalized_depth *= 3\n                depth_frame = (\n                    np.repeat(np.expand_dims(normalized_depth, 2), 3, axis=2) / 3\n                )\n                depth_frame = cv2.applyColorMap(\n                    np.uint8(depth_frame), cv2.COLORMAP_INFERNO\n                )\n                height, width = depth_frame.shape[:2]\n                depth_frame_4 = np.dstack(\n                    [depth_frame, np.ones((height, width), dtype="uint8") * 255]\n                )\n\n                send_output(\n                    "depth_frame",\n                    depth_frame_4.tobytes(),\n                    dora_input["metadata"],\n                )\n        return DoraStatus.CONTINUE\n\n\n'})})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(96540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);